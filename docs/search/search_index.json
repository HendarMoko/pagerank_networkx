{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx 1. scrapy scrapy menurut wikipedia Scrapy (/\u02c8skre\u026api/ skray-pee) adalah framework gratis dan opensource python yang di desain untuk web scrapping dan juga bisa digunakan untuk mengekstrak data menggunakan API maupun seperti web crawler lain pada umumnya , saat ini scrapy dikelola oleh Scrapinghub Ltd. 1.1 pemasangan scrapy pastikan sudah memasang python versi terbaru dan melakukan centang pada pip agar bisa menggunakan perintah pip di cmd setelah memasang python , maka install scrapy mengggunakan perintah pip install scrapy jika gagal menginstall scrapy silahkan install terlebih dahulu yang diminta scrapy bisa saja .net framework ataupun visual studio 14 tergantung keadaan komputer masing masing 1.2 membuat project scrapy saya menggunakan project scrapy yang sama dengan tutorial sebelumnya , silahkan cek tutorial berikut \"\" 1.3 mengatur domain domain digunakan untuk membatasi lingkup situs yang dijelajahi oleh crawler , buka file tugasAkhir.py yang berada di folder spiders lalu atur allowed_domains = ['id.wikipedia.org'] untuk membatasi link yang dijelajahi crawler tetap berada di domain id.wikipedia.org 1.4 mengatur startPage startpage berguna memberi tahu crawler link awal tempat dia mulai menjelajah seperti berikut start_urls = ['https://id.wikipedia.org/wiki/Bank_Indonesia'] di skenario ini saya akan melakukan crawl link internal wikipedia dan akan dibuat graphnya untuk menghitung page rank masing masing halaman 1.5 mengatur data yang akan diambil data yang akan diambil adalah link internal wikipedia indonesia yang akan disusun menjadi graph , karena saya tidak mengatur rule pada project ini maka kita gunakan method default scrapy yaitu parse disini saya juga menggunakan bantuan beautifulsoup untuk antuan extraksi tag html , dan juga link wikipedia perlu di perbaiki karena jika di crawl langsung disimpan akan seperti ini maka diperlukan perbaikan link terlebih dahulu sebelum disimpan dan dijelajahi def parse(self, response): global situsKe situsKe+=1 linkKe = 0 deep = 1 soup = BeautifulSoup(response.body , features=\"lxml\") #inisialisasi beatifulsoup print(\"situsKe = \",situsKe ,\" url = \",response.url) links = soup.find_all('a') #select seluruh tag <a></a> di html linkDiperbaiki=list() #membuat list untuk memperbaiki link di wikipedia for x in links: tmp = str(x.get('href')) if((\"/wiki/\" in tmp[0:6]) and (\":\" not in tmp)): #melakukan pengecekan apakah item link sesuai kriteria yaitu awalannya \"/wiki/\" karena inilah link internal yang akan dicari , dan tidak terdapat char \":\" karena ini indikasi dia merujuk ke halaman itu sendiri tmp1 = str(\"https://id.wikipedia.org\" + tmp) #perbaikan link dengan menambahkan domain linkDiperbaiki.append(tmp1) print(\"panjang links = \",len(linkDiperbaiki)) for x in linkDiperbaiki: #proses simpan link linkKe+=1 print(\"linkKe = \",linkKe,\"url = \",x) item = ItemTugasAkhir() item['url'] = response.url item['link_keluar'] = x item['situsKe'] = situsKe item['linkKe'] = linkKe item['deep'] = deep yield item for x in linkDiperbaiki: # menjelajahi link tsb next_page = response.urljoin(x) yield scrapy.Request(next_page, callback=self.parse_deep2) print(\"===============================end deep1===========================\") copy sampai 3 def parse3 agar crawler kita menjelajah sampai 3 kedalaman 1.6 crawl data untuk crawling data bisa menggunakan perintah scrapy crawl --nolog TugasAkhir -o data.csv -t csv data hasil crawling akan disimpan di data.csv proses crawling seperti berikut tunggu hingga selesai perintah sama dengan tutorial sebelumnya karena memang ini project sebellumnya yang dimodifikasi setelah selesai data akan menjadi seperti berikut : 2. migrasi csv ke sqlite saya menggunakan tools db browser for sqlite disini sangat mudah klik new database ketika muncul pop up untuk mengisi field apa saja yang ada di db close saja setelah itu di menu file pilih import table form csv lali pilih file csv tadi dan lakukan proses migrasi tunggu hingga proses selesai 3. membuat graph menggunakan networkx saya asumsikan data yang akan kita olah sudah ada di sqlite , saya membuat file bernama networkx.py yang digunakan untuk proses pembuatan graph dan menghitung pagerank import networkx as nx import matplotlib.pyplot as plt import sqlite3 def koneksi(db_file): try: conn = sqlite3.connect(db_file) return conn except Error as e: print(e) return None def main(): graph=nx.Graph() #inisialisasi networkx node = set() #inisialisasi node database = \"data.sqlite\" # inisialisasi database dan membuat koneksi conn = koneksi(database) 3.1 menambahkan node node adalah tiap halaman yang dikunjungi maupun link yang terdapat di halaman tsb , link yang terdapat di suatu halaman harus juga dibuat node nya agar saat penambahan edge tidak terjadi kesalahan karena edge merujuk ke suatu node yang tidak di inisialisasi def ambilNode(conn): node = set() cur = conn.cursor() cur.execute(\"select DISTINCT link_keluar from data\") # mengambil link yang terkandung diseluruh halaman yang dikunjungi rows = cur.fetchall() for row in rows: tmp = str(row[0]) node.add(tmp) cur = conn.cursor() cur.execute(\"select DISTINCT url from data\") # mengambil halaman yang telah dikunjungi rows = cur.fetchall() for row in rows: tmp = str(row[0]) node.add(tmp) return node method diatas panggil di main dengan parameter koneksi tadi with conn: node = ambilNode(conn) graph.add_nodes_from(node) 3.2 menambahkan edge menurut dokumentasi networkx untuk menambahkan edge bisa menggunakan method add_edges_from() dengan parameter list yang berisi pasangan-pasangan node yang terhubung misalnya node a node b node c untuk menghubungkan ketiga node list haruslah berbentuk [[node a , node b],[node b , node c],[node c , node a]] def ambilEdge(conn): kumpulanEdge = list() primNode = list() cur = conn.cursor() cur.execute(\"select DISTINCT url from data\") #select seluruh halaman yang pernah dikunjungi rows = cur.fetchall() for row in rows: tmp = str(row[0]) primNode.append(tmp) for x in primNode: edge = list() cur = conn.cursor() query = \"select link_keluar from data where url='\"+str(x)+\"'\" #select seluruh link yang terkandung di masing masing halaman yang pernah dikunjungi cur.execute(query) rows = cur.fetchall() for row in rows: tmp =[str(x),str(row[0])] edge.append(tmp) #menambahkan pasangan - pasangan node kumpulanEdge.append(edge)#menambahkan seluruh pasangan suatu node ke Kumpulan edge , nantinya KumpulanEdge akan berisi seluruh node yang terhubung ke seluruh masing masing node return kumpulanEdge fungsi tadi dipanggil di main dengan parameter koneksi , sehingga keseluruhan fungsi main seperti berikut , karena Kumpulanedge tadi berisi kumpulan maka harus dilakukan perulangan agar bisa menambahkan edge masing-masing node def main(): graph=nx.Graph() node = set() database = \"data.sqlite\" conn = koneksi(database) with conn: node = ambilNode(conn) kumpulanEdge = ambilEdge(conn) graph.add_nodes_from(node) for x in kumpulanEdge : # print(\"===== edge =====\" , kumpulanEdge[x]) graph.add_edges_from(x) 3.3 menampilkan graph untuk menampilkan graph dibutuhkan matplotlib library bantuan untuk menampilkan graph cukup dengan fungsi berikut nx.draw(graph) plt.show() maka graph akan tampil 4. menghitung pagerank dengan networkx dari graph yang telah disusun sebelumnya kita bisa menghitung pagerank menggunkan code berikut pr = nx.pagerank(graph) #menghitung pagerank sorted_pr = sorted(pr.items() , reverse = True, key=lambda kv: kv[1]) # mengurutkan pagerank dari nilai yang terbesar print(\"========== Top 3 pagerank ==========\") #mencetak 3 pagerank tertinggi print(\"1. \", sorted_pr[0]) print(\"2. \", sorted_pr[1]) print(\"3. \", sorted_pr[2]) full code ada di repo berikut :","title":"**Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx**"},{"location":"#tutorial-crawler-link-wikipedia-menggunakan-scrapy-dan-menghitung-pagerank-menggunakan-networkx","text":"","title":"Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx"},{"location":"#1-scrapy","text":"scrapy menurut wikipedia Scrapy (/\u02c8skre\u026api/ skray-pee) adalah framework gratis dan opensource python yang di desain untuk web scrapping dan juga bisa digunakan untuk mengekstrak data menggunakan API maupun seperti web crawler lain pada umumnya , saat ini scrapy dikelola oleh Scrapinghub Ltd.","title":"1. scrapy"},{"location":"#11-pemasangan-scrapy","text":"pastikan sudah memasang python versi terbaru dan melakukan centang pada pip agar bisa menggunakan perintah pip di cmd setelah memasang python , maka install scrapy mengggunakan perintah pip install scrapy jika gagal menginstall scrapy silahkan install terlebih dahulu yang diminta scrapy bisa saja .net framework ataupun visual studio 14 tergantung keadaan komputer masing masing","title":"1.1 pemasangan scrapy"},{"location":"#12-membuat-project-scrapy","text":"saya menggunakan project scrapy yang sama dengan tutorial sebelumnya , silahkan cek tutorial berikut \"\"","title":"1.2 membuat project scrapy"},{"location":"#13-mengatur-domain","text":"domain digunakan untuk membatasi lingkup situs yang dijelajahi oleh crawler , buka file tugasAkhir.py yang berada di folder spiders lalu atur allowed_domains = ['id.wikipedia.org'] untuk membatasi link yang dijelajahi crawler tetap berada di domain id.wikipedia.org","title":"1.3 mengatur domain"},{"location":"#14-mengatur-startpage","text":"startpage berguna memberi tahu crawler link awal tempat dia mulai menjelajah seperti berikut start_urls = ['https://id.wikipedia.org/wiki/Bank_Indonesia'] di skenario ini saya akan melakukan crawl link internal wikipedia dan akan dibuat graphnya untuk menghitung page rank masing masing halaman","title":"1.4 mengatur startPage"},{"location":"#15-mengatur-data-yang-akan-diambil","text":"data yang akan diambil adalah link internal wikipedia indonesia yang akan disusun menjadi graph , karena saya tidak mengatur rule pada project ini maka kita gunakan method default scrapy yaitu parse disini saya juga menggunakan bantuan beautifulsoup untuk antuan extraksi tag html , dan juga link wikipedia perlu di perbaiki karena jika di crawl langsung disimpan akan seperti ini maka diperlukan perbaikan link terlebih dahulu sebelum disimpan dan dijelajahi def parse(self, response): global situsKe situsKe+=1 linkKe = 0 deep = 1 soup = BeautifulSoup(response.body , features=\"lxml\") #inisialisasi beatifulsoup print(\"situsKe = \",situsKe ,\" url = \",response.url) links = soup.find_all('a') #select seluruh tag <a></a> di html linkDiperbaiki=list() #membuat list untuk memperbaiki link di wikipedia for x in links: tmp = str(x.get('href')) if((\"/wiki/\" in tmp[0:6]) and (\":\" not in tmp)): #melakukan pengecekan apakah item link sesuai kriteria yaitu awalannya \"/wiki/\" karena inilah link internal yang akan dicari , dan tidak terdapat char \":\" karena ini indikasi dia merujuk ke halaman itu sendiri tmp1 = str(\"https://id.wikipedia.org\" + tmp) #perbaikan link dengan menambahkan domain linkDiperbaiki.append(tmp1) print(\"panjang links = \",len(linkDiperbaiki)) for x in linkDiperbaiki: #proses simpan link linkKe+=1 print(\"linkKe = \",linkKe,\"url = \",x) item = ItemTugasAkhir() item['url'] = response.url item['link_keluar'] = x item['situsKe'] = situsKe item['linkKe'] = linkKe item['deep'] = deep yield item for x in linkDiperbaiki: # menjelajahi link tsb next_page = response.urljoin(x) yield scrapy.Request(next_page, callback=self.parse_deep2) print(\"===============================end deep1===========================\") copy sampai 3 def parse3 agar crawler kita menjelajah sampai 3 kedalaman","title":"1.5 mengatur data yang akan diambil"},{"location":"#16-crawl-data","text":"untuk crawling data bisa menggunakan perintah scrapy crawl --nolog TugasAkhir -o data.csv -t csv data hasil crawling akan disimpan di data.csv proses crawling seperti berikut tunggu hingga selesai perintah sama dengan tutorial sebelumnya karena memang ini project sebellumnya yang dimodifikasi setelah selesai data akan menjadi seperti berikut :","title":"1.6 crawl data"},{"location":"#2-migrasi-csv-ke-sqlite","text":"saya menggunakan tools db browser for sqlite disini sangat mudah klik new database ketika muncul pop up untuk mengisi field apa saja yang ada di db close saja setelah itu di menu file pilih import table form csv lali pilih file csv tadi dan lakukan proses migrasi tunggu hingga proses selesai","title":"2. migrasi csv ke sqlite"},{"location":"#3-membuat-graph-menggunakan-networkx","text":"saya asumsikan data yang akan kita olah sudah ada di sqlite , saya membuat file bernama networkx.py yang digunakan untuk proses pembuatan graph dan menghitung pagerank import networkx as nx import matplotlib.pyplot as plt import sqlite3 def koneksi(db_file): try: conn = sqlite3.connect(db_file) return conn except Error as e: print(e) return None def main(): graph=nx.Graph() #inisialisasi networkx node = set() #inisialisasi node database = \"data.sqlite\" # inisialisasi database dan membuat koneksi conn = koneksi(database)","title":"3. membuat graph menggunakan networkx"},{"location":"#31-menambahkan-node","text":"node adalah tiap halaman yang dikunjungi maupun link yang terdapat di halaman tsb , link yang terdapat di suatu halaman harus juga dibuat node nya agar saat penambahan edge tidak terjadi kesalahan karena edge merujuk ke suatu node yang tidak di inisialisasi def ambilNode(conn): node = set() cur = conn.cursor() cur.execute(\"select DISTINCT link_keluar from data\") # mengambil link yang terkandung diseluruh halaman yang dikunjungi rows = cur.fetchall() for row in rows: tmp = str(row[0]) node.add(tmp) cur = conn.cursor() cur.execute(\"select DISTINCT url from data\") # mengambil halaman yang telah dikunjungi rows = cur.fetchall() for row in rows: tmp = str(row[0]) node.add(tmp) return node method diatas panggil di main dengan parameter koneksi tadi with conn: node = ambilNode(conn) graph.add_nodes_from(node)","title":"3.1 menambahkan node"},{"location":"#32-menambahkan-edge","text":"menurut dokumentasi networkx untuk menambahkan edge bisa menggunakan method add_edges_from() dengan parameter list yang berisi pasangan-pasangan node yang terhubung misalnya node a node b node c untuk menghubungkan ketiga node list haruslah berbentuk [[node a , node b],[node b , node c],[node c , node a]] def ambilEdge(conn): kumpulanEdge = list() primNode = list() cur = conn.cursor() cur.execute(\"select DISTINCT url from data\") #select seluruh halaman yang pernah dikunjungi rows = cur.fetchall() for row in rows: tmp = str(row[0]) primNode.append(tmp) for x in primNode: edge = list() cur = conn.cursor() query = \"select link_keluar from data where url='\"+str(x)+\"'\" #select seluruh link yang terkandung di masing masing halaman yang pernah dikunjungi cur.execute(query) rows = cur.fetchall() for row in rows: tmp =[str(x),str(row[0])] edge.append(tmp) #menambahkan pasangan - pasangan node kumpulanEdge.append(edge)#menambahkan seluruh pasangan suatu node ke Kumpulan edge , nantinya KumpulanEdge akan berisi seluruh node yang terhubung ke seluruh masing masing node return kumpulanEdge fungsi tadi dipanggil di main dengan parameter koneksi , sehingga keseluruhan fungsi main seperti berikut , karena Kumpulanedge tadi berisi kumpulan maka harus dilakukan perulangan agar bisa menambahkan edge masing-masing node def main(): graph=nx.Graph() node = set() database = \"data.sqlite\" conn = koneksi(database) with conn: node = ambilNode(conn) kumpulanEdge = ambilEdge(conn) graph.add_nodes_from(node) for x in kumpulanEdge : # print(\"===== edge =====\" , kumpulanEdge[x]) graph.add_edges_from(x)","title":"3.2 menambahkan edge"},{"location":"#33-menampilkan-graph","text":"untuk menampilkan graph dibutuhkan matplotlib library bantuan untuk menampilkan graph cukup dengan fungsi berikut nx.draw(graph) plt.show() maka graph akan tampil","title":"3.3 menampilkan graph"},{"location":"#4-menghitung-pagerank-dengan-networkx","text":"dari graph yang telah disusun sebelumnya kita bisa menghitung pagerank menggunkan code berikut pr = nx.pagerank(graph) #menghitung pagerank sorted_pr = sorted(pr.items() , reverse = True, key=lambda kv: kv[1]) # mengurutkan pagerank dari nilai yang terbesar print(\"========== Top 3 pagerank ==========\") #mencetak 3 pagerank tertinggi print(\"1. \", sorted_pr[0]) print(\"2. \", sorted_pr[1]) print(\"3. \", sorted_pr[2]) full code ada di repo berikut :","title":"4. menghitung pagerank dengan networkx"}]}