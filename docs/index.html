



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.2.0">
    
    
      
        <title>My Docs</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/application.750b69bd.css">
      
        <link rel="stylesheet" href="assets/stylesheets/application-palette.224b79ff.css">
      
      
        
        
        <meta name="theme-color" content="#2196f3">
      
    
    
      <script src="assets/javascripts/modernizr.74668098.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="blue" data-md-color-accent="light-blue">
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#tutorial-crawler-link-wikipedia-menggunakan-scrapy-dan-menghitung-pagerank-menggunakan-networkx" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="." title="My Docs" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              zainal arif
            </span>
            <span class="md-header-nav__topic">
              Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="." title="My Docs" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    My Docs
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        **Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx**
      </label>
    
    <a href="." title="**Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx**" class="md-nav__link md-nav__link--active">
      Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx
    </a>
    <a href="https://ariefzzz.github.io/penambanganweb-crawler-Kmean/" title="**Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx**" class="md-nav__link md-nav__link--active">
      Tutorial crawler menggunakan scrapy dan clushtering dokumen menggunakan k-mean
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-scrapy" title="1. scrapy" class="md-nav__link">
    1. scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-pemasangan-scrapy" title="1.1 pemasangan scrapy" class="md-nav__link">
    1.1 pemasangan scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-membuat-project-scrapy" title="1.2 membuat project scrapy" class="md-nav__link">
    1.2 membuat project scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-mengatur-domain" title="1.3 mengatur domain" class="md-nav__link">
    1.3 mengatur domain
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-mengatur-startpage" title="1.4 mengatur startPage" class="md-nav__link">
    1.4 mengatur startPage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-mengatur-data-yang-akan-diambil" title="1.5 mengatur data yang akan diambil" class="md-nav__link">
    1.5 mengatur data yang akan diambil
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16-crawl-data" title="1.6 crawl data" class="md-nav__link">
    1.6 crawl data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-migrasi-csv-ke-sqlite" title="2. migrasi csv ke sqlite" class="md-nav__link">
    2. migrasi csv ke sqlite
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-membuat-graph-menggunakan-networkx" title="3. membuat graph menggunakan networkx" class="md-nav__link">
    3. membuat graph menggunakan networkx
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-menambahkan-node" title="3.1 menambahkan node" class="md-nav__link">
    3.1 menambahkan node
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-menambahkan-edge" title="3.2 menambahkan edge" class="md-nav__link">
    3.2 menambahkan edge
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-menampilkan-graph" title="3.3 menampilkan graph" class="md-nav__link">
    3.3 menampilkan graph
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-menghitung-pagerank-dengan-networkx" title="4. menghitung pagerank dengan networkx" class="md-nav__link">
    4. menghitung pagerank dengan networkx
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-scrapy" title="1. scrapy" class="md-nav__link">
    1. scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-pemasangan-scrapy" title="1.1 pemasangan scrapy" class="md-nav__link">
    1.1 pemasangan scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-membuat-project-scrapy" title="1.2 membuat project scrapy" class="md-nav__link">
    1.2 membuat project scrapy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-mengatur-domain" title="1.3 mengatur domain" class="md-nav__link">
    1.3 mengatur domain
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-mengatur-startpage" title="1.4 mengatur startPage" class="md-nav__link">
    1.4 mengatur startPage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-mengatur-data-yang-akan-diambil" title="1.5 mengatur data yang akan diambil" class="md-nav__link">
    1.5 mengatur data yang akan diambil
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16-crawl-data" title="1.6 crawl data" class="md-nav__link">
    1.6 crawl data
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-migrasi-csv-ke-sqlite" title="2. migrasi csv ke sqlite" class="md-nav__link">
    2. migrasi csv ke sqlite
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-membuat-graph-menggunakan-networkx" title="3. membuat graph menggunakan networkx" class="md-nav__link">
    3. membuat graph menggunakan networkx
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#31-menambahkan-node" title="3.1 menambahkan node" class="md-nav__link">
    3.1 menambahkan node
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#32-menambahkan-edge" title="3.2 menambahkan edge" class="md-nav__link">
    3.2 menambahkan edge
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-menampilkan-graph" title="3.3 menampilkan graph" class="md-nav__link">
    3.3 menampilkan graph
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-menghitung-pagerank-dengan-networkx" title="4. menghitung pagerank dengan networkx" class="md-nav__link">
    4. menghitung pagerank dengan networkx
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="tutorial-crawler-link-wikipedia-menggunakan-scrapy-dan-menghitung-pagerank-menggunakan-networkx"><strong>Tutorial crawler link wikipedia menggunakan scrapy dan menghitung pagerank menggunakan networkx</strong></h1>
<h2 id="1-scrapy"><strong>1. scrapy</strong></h2>
<blockquote>
<p>scrapy menurut <a href="https://en.wikipedia.org/wiki/Scrapy">wikipedia</a> Scrapy (/ˈskreɪpi/ skray-pee) adalah framework gratis dan opensource python yang di desain untuk <em>web scrapping</em> dan juga bisa digunakan untuk mengekstrak data menggunakan API maupun seperti web crawler lain pada umumnya , saat ini scrapy dikelola oleh Scrapinghub Ltd.</p>
</blockquote>
<h2 id="11-pemasangan-scrapy">1.1 pemasangan scrapy</h2>
<p>pastikan sudah memasang <a href="https://www.python.org/">python</a> versi terbaru dan melakukan centang pada pip </p>
<p><img alt="gambar 1" src="assets\images\gambar 1.PNG" /></p>
<p>agar bisa menggunakan perintah pip di cmd</p>
<p>setelah memasang python , maka install scrapy mengggunakan perintah <code>pip install scrapy</code></p>
<p>jika gagal menginstall scrapy silahkan install terlebih dahulu yang diminta scrapy bisa saja .net framework ataupun visual studio 14 tergantung keadaan komputer masing masing </p>
<h2 id="12-membuat-project-scrapy">1.2 membuat project scrapy</h2>
<p>saya menggunakan project scrapy yang sama dengan tutorial sebelumnya , silahkan cek tutorial berikut ""   </p>
<h2 id="13-mengatur-domain">1.3 mengatur domain</h2>
<p>domain digunakan untuk membatasi lingkup situs yang dijelajahi oleh crawler ,</p>
<p>buka file <strong>tugasAkhir.py</strong> yang berada di folder spiders lalu atur <code>allowed_domains = ['id.wikipedia.org']</code> untuk membatasi link yang dijelajahi crawler tetap berada di domain <code>id.wikipedia.org</code></p>
<h2 id="14-mengatur-startpage">1.4 mengatur startPage</h2>
<p>startpage berguna memberi tahu  crawler link awal tempat dia mulai menjelajah seperti berikut <code>start_urls = ['https://id.wikipedia.org/wiki/Bank_Indonesia']</code> di skenario ini saya akan melakukan crawl <strong>link internal</strong> wikipedia dan akan dibuat graphnya untuk menghitung page rank masing masing halaman</p>
<h2 id="15-mengatur-data-yang-akan-diambil">1.5 mengatur data yang akan diambil</h2>
<p>data yang akan diambil adalah link internal wikipedia indonesia yang akan disusun menjadi graph , karena saya tidak mengatur rule pada project ini maka kita gunakan method default scrapy  yaitu  <code>parse</code>  disini saya juga menggunakan bantuan beautifulsoup untuk antuan extraksi tag html , dan juga link wikipedia perlu di perbaiki karena jika di crawl langsung disimpan akan seperti ini </p>
<p><img alt="1558928886876" src="assets\images\1558928886876.png" /></p>
<p>maka diperlukan perbaikan link terlebih dahulu sebelum disimpan dan dijelajahi</p>
<pre><code class="python">    def parse(self, response):
        global situsKe
        situsKe+=1
        linkKe = 0
        deep = 1

        soup = BeautifulSoup(response.body , features=&quot;lxml&quot;)
        #inisialisasi beatifulsoup

        print(&quot;situsKe = &quot;,situsKe ,&quot; url = &quot;,response.url)
        links = soup.find_all('a') #select seluruh tag &lt;a&gt;&lt;/a&gt; di html

        linkDiperbaiki=list() #membuat list untuk memperbaiki link di wikipedia

        for x in links:
            tmp = str(x.get('href'))
            if((&quot;/wiki/&quot; in tmp[0:6]) and (&quot;:&quot; not in tmp)):
            #melakukan pengecekan apakah item link sesuai kriteria yaitu awalannya &quot;/wiki/&quot; karena inilah link internal yang akan dicari , dan tidak terdapat char &quot;:&quot; karena ini indikasi dia merujuk ke halaman itu sendiri 
                tmp1 = str(&quot;https://id.wikipedia.org&quot; + tmp) #perbaikan link dengan menambahkan domain 
                linkDiperbaiki.append(tmp1)

        print(&quot;panjang links  = &quot;,len(linkDiperbaiki))
        for x in linkDiperbaiki:     #proses simpan link    
            linkKe+=1
            print(&quot;linkKe = &quot;,linkKe,&quot;url = &quot;,x)
            item = ItemTugasAkhir()
            item['url'] = response.url
            item['link_keluar'] = x
            item['situsKe'] = situsKe               
            item['linkKe'] = linkKe
            item['deep'] = deep
            yield item
        for x in linkDiperbaiki:  # menjelajahi link tsb 
            next_page = response.urljoin(x)
            yield scrapy.Request(next_page, callback=self.parse_deep2)
        print(&quot;===============================end deep1===========================&quot;)
</code></pre>

<p>copy sampai 3 def  parse3 agar crawler kita menjelajah sampai 3 kedalaman</p>
<h2 id="16-crawl-data">1.6 crawl data</h2>
<p>untuk crawling data bisa menggunakan perintah <code>scrapy crawl --nolog TugasAkhir -o data.csv -t csv</code>  data hasil crawling akan disimpan di data.csv </p>
<p>proses crawling seperti berikut tunggu hingga selesai</p>
<p><strong>perintah sama dengan tutorial sebelumnya karena memang ini project sebellumnya yang dimodifikasi</strong></p>
<p><img alt="1558929375374" src="assets\images\1558929375374.png" /></p>
<p>setelah selesai data akan menjadi seperti berikut :</p>
<p><img alt="1558929728350" src="assets\images\1558929728350.png" /></p>
<h2 id="2-migrasi-csv-ke-sqlite">2. migrasi csv ke sqlite</h2>
<p>saya menggunakan tools <a href="https://sqlitebrowser.org/">db browser for sqlite</a> disini sangat mudah  klik new database ketika muncul  pop up untuk mengisi field apa saja yang ada di db close saja </p>
<p><img alt="1555675137009" src="assets\images\1555675137009.png" /></p>
<p>setelah itu di menu file pilih import table form csv lali pilih file csv tadi dan lakukan proses migrasi</p>
<p><img alt="1558929838634" src="assets\images\1558929838634.png" /></p>
<p>tunggu hingga proses selesai </p>
<p><img alt="1555675509234" src="assets\images\1555675509234.png" /></p>
<h2 id="3-membuat-graph-menggunakan-networkx">3. membuat graph menggunakan networkx</h2>
<p>saya asumsikan data yang akan kita olah sudah ada di sqlite  , saya  membuat file bernama <code>networkx.py</code> yang digunakan untuk proses pembuatan graph dan menghitung pagerank</p>
<pre><code class="python">import networkx as nx
import matplotlib.pyplot as plt
import sqlite3

def koneksi(db_file):
    try:
        conn = sqlite3.connect(db_file)
        return conn
    except Error as e:
        print(e)
    return None
def main(): 
    graph=nx.Graph() #inisialisasi networkx 
    node = set() #inisialisasi node

    database = &quot;data.sqlite&quot; # inisialisasi database dan membuat koneksi
    conn = koneksi(database)



</code></pre>

<h2 id="31-menambahkan-node">3.1 menambahkan node</h2>
<p>node adalah tiap halaman yang dikunjungi maupun link yang terdapat di halaman tsb , link yang terdapat di suatu halaman harus juga dibuat node nya agar saat penambahan edge tidak terjadi kesalahan karena edge merujuk ke suatu node yang tidak di inisialisasi</p>
<pre><code class="python">def ambilNode(conn):
    node = set()

    cur = conn.cursor()
    cur.execute(&quot;select DISTINCT link_keluar from data&quot;) # mengambil link yang terkandung diseluruh halaman yang dikunjungi
    rows = cur.fetchall()
    for row in rows:
        tmp = str(row[0])
        node.add(tmp)

    cur = conn.cursor()
    cur.execute(&quot;select DISTINCT url from data&quot;) # mengambil halaman yang telah dikunjungi
    rows = cur.fetchall()
    for row in rows:
        tmp = str(row[0])
        node.add(tmp)

    return node
</code></pre>

<p>method diatas panggil di main dengan parameter koneksi tadi</p>
<pre><code class="python">with conn: 
        node = ambilNode(conn)
graph.add_nodes_from(node)
</code></pre>

<h2 id="32-menambahkan-edge">3.2 menambahkan edge</h2>
<p>menurut  <a href="https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.Graph.add_edge.html">dokumentasi networkx</a> untuk menambahkan edge bisa menggunakan method <code>add_edges_from()</code> dengan parameter list yang berisi pasangan-pasangan node yang terhubung misalnya </p>
<p>node a</p>
<p>node b</p>
<p>node c</p>
<p>untuk menghubungkan ketiga node list haruslah berbentuk <code>[[node a , node b],[node b , node c],[node c , node a]]</code> </p>
<pre><code class="python">def ambilEdge(conn):
    kumpulanEdge = list()
    primNode = list()

    cur = conn.cursor()
    cur.execute(&quot;select DISTINCT url from data&quot;) #select seluruh halaman yang pernah dikunjungi
    rows = cur.fetchall()
    for row in rows:

        tmp = str(row[0])
        primNode.append(tmp)

    for x in primNode:
        edge = list()
        cur = conn.cursor()
        query =  &quot;select link_keluar from data where url='&quot;+str(x)+&quot;'&quot; #select seluruh link yang terkandung di masing masing halaman yang pernah dikunjungi

        cur.execute(query)
        rows = cur.fetchall()
        for row in rows:
            tmp =[str(x),str(row[0])] 
            edge.append(tmp) #menambahkan pasangan - pasangan node
        kumpulanEdge.append(edge)#menambahkan seluruh pasangan suatu node ke Kumpulan edge , nantinya KumpulanEdge akan berisi seluruh node yang terhubung ke seluruh masing masing node
    return kumpulanEdge
</code></pre>

<p>fungsi tadi dipanggil di main dengan parameter koneksi , sehingga keseluruhan fungsi main seperti berikut , karena Kumpulanedge tadi berisi kumpulan maka harus dilakukan perulangan agar bisa menambahkan edge masing-masing node</p>
<pre><code class="python">def main(): 
    graph=nx.Graph()
    node = set()
    database = &quot;data.sqlite&quot;
    conn = koneksi(database)
    with conn: 
        node = ambilNode(conn)
        kumpulanEdge = ambilEdge(conn)
    graph.add_nodes_from(node)
    for x in kumpulanEdge :
        # print(&quot;===== edge =====&quot; , kumpulanEdge[x])
        graph.add_edges_from(x)
</code></pre>

<h2 id="33-menampilkan-graph">3.3 menampilkan graph</h2>
<p>untuk menampilkan graph dibutuhkan matplotlib library bantuan untuk menampilkan graph cukup dengan fungsi berikut</p>
<pre><code class="python">nx.draw(graph)
plt.show()
</code></pre>

<p>maka graph akan tampil</p>
<p><img alt="1559223397569" src="assets\images\1559223397569.png" /></p>
<h2 id="4-menghitung-pagerank-dengan-networkx">4. menghitung pagerank dengan networkx</h2>
<p>dari  graph yang telah disusun sebelumnya kita bisa menghitung pagerank menggunkan code berikut</p>
<pre><code class="python">pr =  nx.pagerank(graph) #menghitung pagerank 
sorted_pr = sorted(pr.items() , reverse = True, key=lambda kv: kv[1]) # mengurutkan pagerank dari nilai yang terbesar

print(&quot;========== Top 3 pagerank ==========&quot;) #mencetak 3 pagerank tertinggi
print(&quot;1. &quot;, sorted_pr[0])
print(&quot;2. &quot;, sorted_pr[1])
print(&quot;3. &quot;, sorted_pr[2])

</code></pre>

<p><img alt="1559226072683" src="assets\images\1559226072683.png" /></p>
<p>full code ada di repo berikut  : <a href="https://github.com/ariefzzz/pagerank_networkx/tree/master/source">pagerank_networkx</a> </p>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="assets/javascripts/application.39abc4af.js"></script>
      
      <script>app.initialize({version:"1.0.4",url:{base:"."}})</script>
      
    
  </body>
</html>